{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from eloguessr import EloGuessr\n",
    "from utils import load_data, plot_losses\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Home/siv33/vbo084/EloGuessr/data/processed'\n",
    "model_dir = '/Home/siv33/vbo084/EloGuessr/data/processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/Home/siv33/vbo084/EloGuessr/data/processed' + 'model.pt')\n",
    "\n",
    "fnames = ['chess_train_elite.pt', 'chess_val_elite.pt', 'chess_test_elite.pt']\n",
    "BATCH_SIZE = 128\n",
    "val_dloader, test_dloader = load_data(data_dir, fnames, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_scores = []\n",
    "true_scores = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_dloader:\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        scores = model(inputs)\n",
    "        pred_scores.append(scores.squeeze())  # Ensure scores are in the right shape\n",
    "        true_scores.append(targets.squeeze())  # Ensure targets are in the right shape\n",
    "\n",
    "\n",
    "pred_scores = torch.cat(pred_scores, dim=0)\n",
    "true_scores = torch.cat(true_scores, dim=0)\n",
    "assert pred_scores.shape == true_scores.shape, f\"Shape mismatch: {pred_scores.shape} vs {true_scores.shape}\"\n",
    "\n",
    "hits = torch.abs(pred_scores - true_scores) <= 100\n",
    "hit_percentage = hits.sum().item() / len(hits) * 100\n",
    "\n",
    "print(f'Hit Percentage: {hit_percentage:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model_list: list, dataloader):\n",
    "    pred_scores = []\n",
    "    true_scores = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            scores = model(inputs)\n",
    "            pred_scores.append(scores.squeeze())  # Ensure scores are in the right shape\n",
    "            true_scores.append(targets.squeeze())  # Ensure targets are in the right shape\n",
    "\n",
    "\n",
    "    pred_scores = torch.cat(pred_scores, dim=0)\n",
    "    true_scores = torch.cat(true_scores, dim=0)\n",
    "    assert pred_scores.shape == true_scores.shape, f\"Shape mismatch: {pred_scores.shape} vs {true_scores.shape}\"\n",
    "\n",
    "    hits = torch.abs(pred_scores - true_scores) <= 100\n",
    "    hit_percentage = hits.sum().item() / len(hits) * 100\n",
    "\n",
    "    print(f'Hit Percentage: {hit_percentage:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
